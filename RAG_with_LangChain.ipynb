{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KJjbIFJX3s26"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZqTADnyr6kJ8",
        "outputId": "2cdf66b0-93b8-413b-abfa-ca02bcfb3da0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.61 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.63)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.82.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.3.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.61->langchain-openai) (2.11.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.61->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.61->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are using multiple LLMs and want to debug, test, and monitor\n",
        "# AI app performance, we can use LagGmith.\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46myPQzm6siQ",
        "outputId": "d288f634-77d4-43cf-c3e5-261ddffdbcfb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import chat model from langchain\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
      ],
      "metadata": {
        "id": "9c5QefUH61h-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import embeddings class and intialize an embeddings object\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "asI1M26465wl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the vector store and initialize it with the embeddings model\n",
        "\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "AOqKIicHFHC7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Building a simple indexing pipeline and RAG chain to create an app that answers questions about the website's content, following the LANGCHAIN documentation\n"
      ],
      "metadata": {
        "id": "nP3M3sW37ZJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Load the web content and split it into chunks\n",
        "\n",
        "# Load from a specific web path\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://python.langchain.com/docs/tutorials/rag/\",),\n",
        "          bs_kwargs={\"parse_only\": bs4.SoupStrainer(class_=\"main-content\")},\n",
        "  )\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "# Split the text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=200)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Define a few sentences as the web content for a quick check\n",
        "simple_documents = [\n",
        "    Document(page_content=\"Overview\"),\n",
        "    Document(page_content=\"A typical RAG application has two main components:\"),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "# Index chunks of text in the vector store\n",
        "_ = vector_store.add_documents(simple_documents)"
      ],
      "metadata": {
        "id": "xEjjdZ5iGjQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define question prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Define application state\n",
        "class State(TypedDict):\n",
        "  \"\"\"\n",
        "  Represents the application state as a dictionary with a specific keys:\n",
        "\n",
        "  keys:\n",
        "      question (str): the input question as a string.\n",
        "      context (List[Document]): a list of documents representing the relevant context.\n",
        "      answer (str): the generated or retrieved answer as a string.\n",
        "\n",
        "  \"\"\"\n",
        "  question: str\n",
        "  context: List[Document]\n",
        "  answer: str\n"
      ],
      "metadata": {
        "id": "39BOG4989qZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define application steps\n",
        "\n",
        "# 1- Define retrieve function\n",
        "def retrieve(state: State):\n",
        "  \"\"\"\n",
        "  Performs retrieval of relevant documents from the vector database based on the user's question.\n",
        "\n",
        "  Parameters:\n",
        "      state: A dictionary containing at least the 'question' key with the user's query as the value.\n",
        "\n",
        "  Returns:\n",
        "      context: A list of documents representing the relevant context.\n",
        "  \"\"\"\n",
        "  retrieved_documents = vector_store.similarity_search(state[\"question\"])\n",
        "  return {\"context\": retrieved_documents}\n",
        "\n",
        "\n",
        "# 2- Define generate function\n",
        "\n",
        "def generate(state: State):\n",
        "  \"\"\"\n",
        "  Performs generation based on the user's question and the relevant context.\n",
        "\n",
        "  Parameters:\n",
        "      state: A dictionary containing:\n",
        "          - 'question': The user's query as a string.\n",
        "          - 'context': The relevant context (e.g., a list of retrieved documents).\n",
        "\n",
        "  Returns:\n",
        "      answer: The generated or retrieved answer as a string.\n",
        "  \"\"\"\n",
        "  documnets_content = \"\\n\\n\".join([doc.page_content for doc in state[\"context\"]])\n",
        "  prompt_with_context = prompt.invoke({\"question\": state[\"question\"], \"context\": documnets_content})\n",
        "  response = llm.invoke(prompt_with_context)\n",
        "  return {\"answer\": response.content}"
      ],
      "metadata": {
        "id": "pyZNavp8_mOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run application\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "G1-Wiwm6-jn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "id": "Qbn2u50AAoxX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}